<!-- $theme: gaia -->

# Monte Carlo Methods
<!-- *template: invert -->
- 预测==值函数==和发现==最优策略==的第一个学习方法.
- 蒙特卡罗方法只需要==在线的==或者与环境==仿真==交互的关于==状态==，==动作==，==奖赏==的样本序列.
  sample sequences of states, actions, and rewards from actual or simulated interaction with an environment.
  - 在线学习不需要动态环境的==先验知识==，仍然可以获得最优行为(optimal behavior). 
  - 仿真学习仅需要==generate sample transitions==, not the complete probability distributions of all possible transitions that is required for DP. 
---
<!-- page_number: true -->
<!-- $theme:gaia -->
<!-- *template: invert -->
- 蒙特卡罗方法是基于==averaging sample returns==解决强化问题的一种方法.
- 假设==experience==被分成了片段(episodes),无论采取任何动作任何一个片段最终都会==结束==。
- Monte Carlo methods can be ==incremental== in an ==episode-by-episode sense==, but not in a step-by-step (online) sense.
- 当返回值(==returns==)越来越多时, 其平均值收敛到期望值. 这就是蒙特卡罗蕴含的思想.
---
<!-- $theme:gaia -->
<!-- *template: invert -->
# Monte Carlo Prediction
- 在一个片段中状态 $s$ 的每一次发生都称为对$s$的一次访问.
- 同一个片段 $s$ 可能访问多次; 称在一个片段中第一次访问为对 $s$ 的首次访问.
- first-visit MC 对$s$首次访问返回值取平均来估计$v_\pi(s)$.
- every-visit MC 对 $s$ 的所有访问的放回值取平均来估计$v_\pi(s)$.